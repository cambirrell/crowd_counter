{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowd Counter Model\n",
    "Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, Lambda, ToTensor\n",
    "import warnings\n",
    "# Suppress the specific UserWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"The default value of the antialias parameter.*\", category=UserWarning)\n",
    "# %env CUDA_VISIBLE_DEVICES=5,6,7\n",
    "device = torch.device(\"cuda:10\" if torch.cuda.is_available() else torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for the images (you can customize this based on your needs)\n",
    "test_size = (512,512)\n",
    "out_size = (64,64)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(test_size),\n",
    "])\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CrowdDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='jhu_crowd_v2.0/', split='train', transform=None):\n",
    "        self.input_folder = os.path.join(root, split, 'images')\n",
    "        self.output_folder = os.path.join(root, split, 'den')\n",
    "        self.input_dataset = datasets.ImageFolder(self.input_folder, transform=transform)\n",
    "        self.classes = self.input_dataset.classes\n",
    "        self.indices = list(range(len(self.input_dataset)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load input image\n",
    "        input_data = self.input_dataset[self.indices[index]][0]  # [0] to get the data (image)\n",
    "\n",
    "        # Load output data from CSV file\n",
    "        image_name = os.path.basename(self.input_dataset.imgs[self.indices[index]][0])\n",
    "        csv_path = os.path.join(self.output_folder, f\"{image_name.replace('.jpg', '.csv')}\")\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "        original_data = torch.tensor(df.values).float()\n",
    "        # check if any values are nan\n",
    "        if torch.isnan(original_data).any():\n",
    "            print(f\"NaN found in {csv_path}\")\n",
    "            # original_data[torch.isnan(original_data)] = 0\n",
    "        resized_tensor = F.interpolate(original_data.unsqueeze(0).unsqueeze(0), size=out_size, mode='bilinear', align_corners=False)\n",
    "        # check if any values are nan\n",
    "        if torch.isnan(resized_tensor).any():\n",
    "            print(f\"NaN found in {csv_path}\")\n",
    "            print(\"error in interpolation\")\n",
    "            # resized_tensor[torch.isnan(resized_tensor)] = 0\n",
    "        resized_tensor = resized_tensor.squeeze(0).squeeze(0)\n",
    "        output_data = resized_tensor * (original_data.sum() / resized_tensor.sum())\n",
    "        if  resized_tensor.sum() == 0:\n",
    "            print(f\"Zero sum found in {csv_path}\")\n",
    "            print(\"error in interpolation\")\n",
    "            # output_data[torch.isnan(output_data)] = 0\n",
    "        # check if any values are nan\n",
    "        if torch.isnan(output_data).any():\n",
    "            print(f\"NaN found in {csv_path}\")\n",
    "            print(\"error in regularization\")\n",
    "            output_data[torch.isnan(output_data)] = 0\n",
    "        return {'input': input_data, 'output': output_data}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrebBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(UrebBlock, self).__init__()\n",
    "        self.CB = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # Add padding to keep the spatial dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),   # Add padding to keep the spatial dimensions\n",
    "        )\n",
    "        self.DR = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # Add padding to keep the spatial dimensions\n",
    "        )\n",
    "        self.CEB = nn.Sequential(\n",
    "            nn.Conv2d(33, 32, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),  # Add padding to keep the spatial dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),   # Add padding to keep the spatial dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1),   # Add padding to keep the spatial dimensions\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        r = self.CB(x)\n",
    "        cm = self.CEB(torch.cat((r, self.DR(x)), dim=1))\n",
    "        return cm * r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGadjusted(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGadjusted, self).__init__()\n",
    "        self.vgg = models.vgg16(weights=None).features\n",
    "        self.C3 = self.vgg[:17]\n",
    "        self.C4 = self.vgg[17:24]\n",
    "        self.C5 = self.vgg[24:]\n",
    "        self.C6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 32, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # Add padding to keep the spatial dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),   # Add padding to keep the spatial dimensions\n",
    "            nn.Upsample(size=(16, 16), mode='bilinear', align_corners=False)\n",
    "        )\n",
    "        self.ureb3 = UrebBlock(256)\n",
    "        self.ureb4 = UrebBlock(512)\n",
    "        self.ureb5 = UrebBlock(512)\n",
    "        self.y5_upsample = nn.Upsample(size=(32, 32), mode='bilinear', align_corners=False)\n",
    "        self.y4_upsample = nn.Upsample(size=(64, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x', torch.isnan(x).any())\n",
    "        c3 = self.C3(x)\n",
    "        # print('c3', torch.isnan(c3).any())\n",
    "        r3 = self.ureb3(c3)\n",
    "        # print('r3', torch.isnan(r3).any())\n",
    "        c4 = self.C4(c3)\n",
    "        # print('c4', torch.isnan(c4).any())\n",
    "        r4 = self.ureb4(c4)\n",
    "        # print('r4', torch.isnan(r4).any())\n",
    "        c5 = self.C5(c4)\n",
    "        # print('c5', torch.isnan(c5).any())\n",
    "        r5 = self.ureb5(c5)\n",
    "        # print('r5', torch.isnan(r5).any())\n",
    "        y6 = self.C6(c5)\n",
    "        # print('y6', torch.isnan(y6).any())\n",
    "        y5 = y6 + r5\n",
    "        # print('y5', torch.isnan(y5).any())\n",
    "        y4 = self.y5_upsample(y5) + r4\n",
    "        # print('y4', torch.isnan(y4).any())\n",
    "        y3 = self.y4_upsample(y4) + r3\n",
    "        # print('y3', torch.isnan(y3).any())\n",
    "        # see if any of these vectors have any nan values\n",
    "        # for i, j in enumerate([y3, y4, y5, y6, r3, r4, r5]):\n",
    "        #     print(i, torch.isnan(j).any())\n",
    "        out = {'y3': y3, 'y4': y4, 'y5': y5, 'y6': y6, 'cm3': r3, 'cm4': r4, 'cm5': r5,}\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.lambda_c = -0.001\n",
    "        self.lambda_d = 10000\n",
    "        self.cm_keys = ['cm3', 'cm4', 'cm5']\n",
    "        self.y_keys = ['y3', 'y4', 'y5']\n",
    "\n",
    "    def _scale_regularize_tensor(self, tensor, out_size):\n",
    "        # print(tensor.shape)\n",
    "        # print(tuple(list(out_size)[1:]))\n",
    "        out_size = tuple(list(out_size)[1:])\n",
    "        # print(tensor.unsqueeze(0).shape)\n",
    "        resized = F.interpolate(tensor.unsqueeze(0), size=out_size, mode='bilinear', align_corners=False)\n",
    "        resized = resized.squeeze(0).squeeze(0)\n",
    "        scaled_tensor = resized * (tensor.sum() / resized.sum())\n",
    "        return scaled_tensor\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        # print(y_hat.keys())\n",
    "        # print('cm failure') if any([torch.isnan(y_hat[i]).any().item() for i in self.cm_keys ]) else print('cm success')\n",
    "        # print('y_hat failure') if any([torch.isnan(y_hat[i]).any().item() for i in self.y_keys ]) else print('y_hat success')\n",
    "        # print('y failure') if torch.isnan(y).any().item() else print('y success')\n",
    "        # print(y_hat['cm3'].shape)\n",
    "        # print(y_hat['cm3'])\n",
    "        std_dev = 0.1\n",
    "        loss_C = sum(torch.log(torch.relu(y_hat[cm_key]) + torch.abs(torch.randn_like(y_hat[cm_key]) * std_dev) ).mean() for cm_key in self.cm_keys)\n",
    "        # print(\"predicted\", (y_hat['cm3'] * y_hat['y3']).shape)\n",
    "        # print(\"actual\", (y_hat['cm3'] * self._scale_regularize_tensor(y, y_hat['cm3'].squeeze(1).shape).unsqueeze(1)).shape)\n",
    "        # print(y_hat['cm3'].shape)\n",
    "        # print(y_hat['y3'].shape)\n",
    "        # print(self._scale_regularize_tensor(y, y_hat['cm3'].squeeze(1).shape).unsqueeze(1).shape)\n",
    "        loss_d = sum(F.mse_loss(y_hat[cm_key] * y_hat[y_key], y_hat[cm_key] * self._scale_regularize_tensor(y, y_hat[cm_key].squeeze(1).shape).unsqueeze(1)) for y_key, cm_key in list(zip(self.y_keys, self.cm_keys)))\n",
    "        # print('loss_C', loss_C)\n",
    "        # print('loss_d', loss_d)\n",
    "        \n",
    "        return self.lambda_d * loss_d + self.lambda_c * loss_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # Initialize distributed training\n",
    "    # torch.distributed.init_process_group(\n",
    "    #     backend='nccl',\n",
    "    #     rank=0,\n",
    "    #     world_size=3  # Total number of processes (GPUs)\n",
    "    # )\n",
    "\n",
    "    # make all the datasets\n",
    "    train_dataset = CrowdDataset(split='train', transform=transform)\n",
    "    val_dataset = CrowdDataset(split='val', transform=None)\n",
    "    test_dataset = CrowdDataset(split='test', transform=None)\n",
    "\n",
    "    #make a dataloader\n",
    "    # train_sampler = DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = VGGadjusted()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = LossFunction()\n",
    "    epochs = 3\n",
    "    loop = tqdm(total=len(train_dataloader)*epochs, position=0, leave=False)\n",
    "    val_loss = []\n",
    "    train_loss = []\n",
    "\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': 0,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': 0.0,\n",
    "    }\n",
    "    # model = DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_data = batch['input'].to(device)\n",
    "            output_data = batch['output'].to(device)\n",
    "            output_pred = model(input_data)\n",
    "            # for key in output_pred.keys():\n",
    "            #     print('key',torch.isnan(output_pred[key]).any().item())\n",
    "            loss = criterion(output_pred, output_data)\n",
    "            loss.backward()\n",
    "            if i % 10 == 0:\n",
    "                train_loss.append(loss.item())\n",
    "                # val_loss.append()\n",
    "            optimizer.step()\n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            loop.update(1)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if torch.isnan(param).any().item():\n",
    "                    print(\"MODEL FAILURE\")\n",
    "\n",
    "        checkpoint['epoch'] = epoch\n",
    "        checkpoint['model_state_dict'] = model.state_dict()\n",
    "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        checkpoint['loss'] = loss.item()\n",
    "        torch.save(checkpoint, f'checkpoint_{epoch}.pth')\n",
    "    loop.close()\n",
    "    # Clean up\n",
    "    # torch.distributed.destroy_process_group()\n",
    "    plt.plot(train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    #evaluate\n",
    "    val_mse = []\n",
    "    checkpoint = torch.load('checkpoint_0.pth')\n",
    "    model = VGGadjusted()\n",
    "    # model = DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    val_dataset = CrowdDataset(split='val', transform=transform)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    criterion = LossFunction()\n",
    "    val_loss = []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        input_data = batch['input'].to(device)\n",
    "        output_data = batch['output'].to(device)\n",
    "        output_pred = model(input_data)\n",
    "        estimate = output_pred['y3'].sum().item()\n",
    "        loss = criterion(output_pred, output_data)\n",
    "        val_mse.append(estimate)\n",
    "        val_loss.append(loss.item())\n",
    "    plt.plot(val_loss)\n",
    "    plt.show()\n",
    "    ground_truth = pd.read_csv('jhu_crowd_v2.0/val/image_labels.txt', header=None)\n",
    "    gt = list(ground_truth[1])\n",
    "    print('MSE: ', sum([(i[1] - i[0])**2 for i in list(zip(gt, val_mse))])/len(gt))\n",
    "    print('MAE: ', sum([abs(i[1] - i[0]) for i in list(zip(gt, val_mse))])/len(gt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:   7%|▋         | 58/852 [01:52<27:34,  2.08s/it, loss=0.0103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero sum found in jhu_crowd_v2.0/train/den/1495.csv\n",
      "error in interpolation\n",
      "NaN found in jhu_crowd_v2.0/train/den/1495.csv\n",
      "error in regularization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:   8%|▊         | 72/852 [02:19<25:02,  1.93s/it, loss=0.0123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero sum found in jhu_crowd_v2.0/train/den/4228.csv\n",
      "error in interpolation\n",
      "NaN found in jhu_crowd_v2.0/train/den/4228.csv\n",
      "error in regularization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:  14%|█▍        | 123/852 [03:47<18:54,  1.56s/it, loss=0.0172] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero sum found in jhu_crowd_v2.0/train/den/3089.csv\n",
      "error in interpolation\n",
      "NaN found in jhu_crowd_v2.0/train/den/3089.csv\n",
      "error in regularization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:  16%|█▌        | 133/852 [04:05<22:55,  1.91s/it, loss=0.00912]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero sum found in jhu_crowd_v2.0/train/den/1564.csv\n",
      "error in interpolation\n",
      "NaN found in jhu_crowd_v2.0/train/den/1564.csv\n",
      "error in regularization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:  19%|█▉        | 163/852 [04:53<17:50,  1.55s/it, loss=0.00893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero sum found in jhu_crowd_v2.0/train/den/1002.csv\n",
      "error in interpolation\n",
      "NaN found in jhu_crowd_v2.0/train/den/1002.csv\n",
      "error in regularization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/3]:  19%|█▉        | 165/852 [04:56<17:43,  1.55s/it, loss=0.00893]"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [64, 64] and output size of (1, 64, 64). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 19\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m output_pred \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[1;32m     18\u001b[0m estimate \u001b[38;5;241m=\u001b[39m output_pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m val_mse\u001b[38;5;241m.\u001b[39mappend(estimate)\n\u001b[1;32m     21\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[65], line 33\u001b[0m, in \u001b[0;36mLossFunction.forward\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_regularize_tensor(y, y_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcm3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 33\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcm_key\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_key\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcm_key\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_regularize_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcm_key\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcm_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcm_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print('loss_C', loss_C)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print('loss_d', loss_d)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_d \u001b[38;5;241m*\u001b[39m loss_d \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_c \u001b[38;5;241m*\u001b[39m loss_C\n",
      "Cell \u001b[0;32mIn[65], line 33\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_regularize_tensor(y, y_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcm3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 33\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(F\u001b[38;5;241m.\u001b[39mmse_loss(y_hat[cm_key]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m y_hat[y_key]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), y_hat[cm_key]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_regularize_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcm_key\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m y_key, cm_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcm_keys)))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print('loss_C', loss_C)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print('loss_d', loss_d)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_d \u001b[38;5;241m*\u001b[39m loss_d \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_c \u001b[38;5;241m*\u001b[39m loss_C\n",
      "Cell \u001b[0;32mIn[65], line 14\u001b[0m, in \u001b[0;36mLossFunction._scale_regularize_tensor\u001b[0;34m(self, tensor, out_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlist\u001b[39m(out_size)[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(tensor.unsqueeze(0).shape)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m resized \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m resized \u001b[38;5;241m=\u001b[39m resized\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m scaled_tensor \u001b[38;5;241m=\u001b[39m resized \u001b[38;5;241m*\u001b[39m (tensor\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m resized\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3869\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3871\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3872\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3873\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3874\u001b[0m \n\u001b[1;32m   3875\u001b[0m         )\n\u001b[1;32m   3876\u001b[0m     output_size \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   3877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [64, 64] and output size of (1, 64, 64). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
